{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjnA3ihtmRz-"
      },
      "outputs": [],
      "source": [
        "pip install transformers torch pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/corpus2 (1).csv')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['label'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Split dataset into train and test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize text\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "F1yOacr7mU5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "class CropDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels.tolist() # Convert labels to a list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CropDataset(train_encodings, train_labels)\n",
        "test_dataset = CropDataset(test_encodings, test_labels)\n",
        "\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ymXgFYiDmmpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the pre-trained model and tokenizer (adjust paths as needed)\n",
        "model = BertForSequenceClassification.from_pretrained('/content/results/checkpoint-880')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Prepare your test dataset\n",
        "inputs = {\n",
        "    'input_ids': torch.tensor(test_dataset.encodings['input_ids']),\n",
        "    'attention_mask': torch.tensor(test_dataset.encodings['attention_mask'])\n",
        "}\n",
        "test_labels = test_dataset.labels  # Access labels from test_dataset\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the predicted class labels\n",
        "preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()"
      ],
      "metadata": {
        "id": "VwnKBYlumyC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, preds)\n",
        "\n",
        "# Calculate precision, recall, and F1 score (use 'weighted' for multi-class classification)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, preds, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "FU7wpVG_nIUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict crop type based on input features\n",
        "def predict_crop(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=1)\n",
        "    return label_encoder.inverse_transform(prediction.cpu().numpy())[0]\n",
        "\n",
        "# Function to get feature values one by one from the user\n",
        "def get_features():\n",
        "    feature_names = [\"N\", \"P\", \"K\", \"temperature\", \"humidity\", \"rainfall\"]\n",
        "    features = []\n",
        "\n",
        "    print(\"Please enter the following features one by one:\")\n",
        "\n",
        "    for feature in feature_names:\n",
        "        while True:\n",
        "            try:\n",
        "                value = float(input(f\"Enter value for {feature}: \"))\n",
        "                features.append(f\" {feature}:{value},\")\n",
        "                break\n",
        "            except ValueError:\n",
        "                print(f\"Invalid input for {feature}. Please enter a numeric value.\")\n",
        "\n",
        "    # Convert features to the string format expected by the tokenizer\n",
        "    return \" \".join(map(str, features))\n",
        "\n",
        "# Chatbot interface\n",
        "def chatbot():\n",
        "    print(\"Welcome to the Crop Recommendation Chatbot!\")\n",
        "    print(\"You will be asked to input various feature values to get a crop recommendation.\")\n",
        "\n",
        "    while True:\n",
        "        text = input(\"Type 'start' to enter features, or 'exit' to quit: \").lower()\n",
        "\n",
        "        if text == 'exit' or text == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        elif text == 'start':\n",
        "            # Get features from user\n",
        "            features = get_features()\n",
        "\n",
        "            # Predict crop based on the features\n",
        "            crop = predict_crop(model, tokenizer, features)\n",
        "            print(f\"Recommended crop: {crop}\")\n",
        "        else:\n",
        "            print(\"Invalid command. Please type 'start' to begin or 'exit' to quit.\")\n",
        "\n",
        "# Run chatbot\n",
        "chatbot()\n"
      ],
      "metadata": {
        "id": "ke-Er4cunLZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}